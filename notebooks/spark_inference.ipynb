{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0VuRgSFO7uJ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Now that we've mounted your Drive, this ensures that\n",
        "# the Python interpreter of the Colab VM can load\n",
        "# python files from within it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRUdTGg8v7Li"
      },
      "source": [
        "### Setting up\n",
        "\n",
        "Change the following in the below cell\n",
        "\n",
        "- `n_infer`: number of videos \n",
        "- `n_partitions` : number of parallel workers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTKAJrJUaiRk"
      },
      "outputs": [],
      "source": [
        "n_infer = 100 # [50, 100, 500, 1000]\n",
        "n_partitions = 3 # [1,2,3,4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f9nI-i3wbrD"
      },
      "source": [
        "### Accessing the directory \n",
        "\n",
        "Google Drive's link for the working directory is `https://drive.google.com/drive/folders/1XFVNSJR3ZiekIigyXs5cGe1mN8JRoeMe?usp=sharing`.\n",
        "\n",
        "To run this notebook, \n",
        "- right-click on the `CLIP-spark` directory\n",
        "- Choose `Add shortcut to drive` (This will create a shortcut in you `My Drive` dir) \n",
        "\n",
        "Then run the below cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEq7YPNOtL1N"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os \n",
        "FOLDERNAME = 'CLIP-spark'\n",
        "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
        "sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
        "\n",
        "%cd /content/drive/My\\ Drive/$FOLDERNAME/data/v/\n",
        "%cd /content/drive/My\\ Drive/$FOLDERNAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUuzr3MYsKQp"
      },
      "source": [
        "### Setting up Java, CLIP, PySpark, MongoDB\n",
        "\n",
        "For the below cell, you need to press `Enter` to finish the installation process. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjJFb7n5sJl_"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Run this cell only 1 time (even if runtime is restarted)\n",
        "'''\n",
        "\n",
        "!sudo add-apt-repository ppa:webupd8team/java\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install oracle-java8-installer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7STGvO0ps_hS"
      },
      "outputs": [],
      "source": [
        "!pip install decord\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install pytube "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGECp8swtARU"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# !rm spark-3.0.1-bin-hadoop2.7.tgz\n",
        "# !wget -q https://archive.apache.org/dist/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.0.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"spark-3.0.1-bin-hadoop2.7\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import array_contains"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-osLmjGxcHa"
      },
      "source": [
        "### Connecting to MongoDB server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qaDQOFAellj"
      },
      "outputs": [],
      "source": [
        "USERNAME = PASSWORD = 'sri'\n",
        "CNCT_STR = f'mongodb+srv://{USERNAME}:{PASSWORD}@svp-cluster.1uzpyjf.mongodb.net/svp_database.video_tags?retryWrites=true'\n",
        "FORMAT = 'com.mongodb.spark.sql.DefaultSource'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmMRhe-tLhbw"
      },
      "outputs": [],
      "source": [
        "videos_dir = '/content/drive/My Drive/{}/data/v'.format(FOLDERNAME)\n",
        "num_videos = len(os.listdir(f'data/v'))\n",
        "print('downloaded videos:', num_videos)\n",
        "# if not os.path.exists(videos_dir):\n",
        "#   os.makedirs(videos_dir)\n",
        "#   print(f'{videos_dir} created!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfH0F9NIw5Xu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import clip "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wb7ESVGGw0pZ"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijCTUa3_2gBW"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, FloatType\n",
        "from pyspark.sql.functions import spark_partition_id\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "      .master(\"local[5]\") \\\n",
        "      .appName(\"inference\") \\\n",
        "      .config(\"spark.driver.memory\", '15g') \\\n",
        "      .config('spark.ui.port', '4050') \\\n",
        "      .config('spark.mongodb.input.uri', CNCT_STR) \\\n",
        "      .config('spark.mongodb.output.uri', CNCT_STR) \\\n",
        "      .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1') \\\n",
        "      .getOrCreate() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knxcaj2L2j-1"
      },
      "outputs": [],
      "source": [
        "video_id_csv = f'downloaded_video_ids_{n_infer}.csv'\n",
        "df_small = spark.read.csv(video_id_csv, header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuRHqpQk2pte"
      },
      "outputs": [],
      "source": [
        "paths_schema = StructType(\n",
        "    [\n",
        "        StructField(\"video_id\", StringType(), True),\n",
        "        StructField(\"video_path\", StringType(), True)\n",
        "    ]\n",
        ")\n",
        "\n",
        "def create_paths(pdf):\n",
        "  ids = pdf.video_id\n",
        "  video_paths = [f'data/v/{row.video_id}' for idx, row in pdf.iterrows()]\n",
        "  return pdf.assign(video_path=video_paths)\n",
        "\n",
        "pdf = df_small.groupby('video_id').applyInPandas(\n",
        "    create_paths, schema=paths_schema\n",
        ")\n",
        "# pdf.show()\n",
        "print('pdf original partitions:', pdf.rdd.getNumPartitions())\n",
        "pdf_with_partition = pdf.coalesce(n_partitions)\n",
        "\n",
        "num_partitions = pdf_with_partition.rdd.getNumPartitions()\n",
        "print('num_partitions:', num_partitions)\n",
        "\n",
        "pdf_with_partition = pdf_with_partition.withColumn('partition', spark_partition_id())\n",
        "pdf_with_partition.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VK-MkgDB2u96"
      },
      "outputs": [],
      "source": [
        "from clip_tagger import CLIPTag\n",
        "def tagging_func(pdf):\n",
        "  # initiate a new clip_tagger\n",
        "  model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "  clip_tagger = CLIPTag(model, preprocess)\n",
        "  all_tags = [clip_tagger.tag_video(row.video_path) for idx, row in pdf.iterrows()]\n",
        "  return pdf.assign(tags=all_tags)\n",
        "\n",
        "tags_schema = StructType(\n",
        "    [\n",
        "        StructField(\"video_id\", StringType(), True),\n",
        "        StructField(\"video_path\", StringType(), True),\n",
        "        StructField(\"partition\", FloatType(), True),\n",
        "        StructField(\"tags\", ArrayType(StringType()), True)\n",
        "    ]\n",
        ")\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "tags_df = pdf_with_partition.groupby(spark_partition_id().alias(\"_pid\")).applyInPandas(\n",
        "    tagging_func, schema=tags_schema\n",
        ")\n",
        "\n",
        "tags_df.show()\n",
        "\n",
        "num_partitions = pdf_with_partition.rdd.getNumPartitions()\n",
        "end = time.time() - start\n",
        "print(f'num_partitions: {num_partitions} time for {n_infer} videos: {end/60:.3f} mins')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOP2SAj93bdp"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Writing tags to the database server.\n",
        "'''\n",
        "\n",
        "try:\n",
        "  tags_df.write.format(FORMAT).mode('overwrite').save()\n",
        "except:\n",
        "  print('Error writing to database.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qt3Q-u0BBXJW"
      },
      "outputs": [],
      "source": [
        "xticks = [100, 200, 300, 400, 500]\n",
        "p1 = [20.98, 41.70, 65.607, 90, 1108.79]\n",
        "p2 = [10.76, 23.62, 32.56, 42.1, 62.104]\n",
        "p3 = [7.64, 13.69, 19.55, 25.44, 38.40]\n",
        "p4 = [12.96, 24.75, 32.75, 46.45, 65.56]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvLmMvJFv2UT"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt \n",
        "import numpy as np \n",
        "\n",
        "xticks = [100, 200, 300, 400, 500]\n",
        "p1 = [20.98, 41.70, 65.607, 90, 108.79]\n",
        "p2 = [10.76, 23.62, 32.56, 42.1, 62.104]\n",
        "p3 = [7.64, 13.69, 19.55, 25.44, 38.40]\n",
        "p4 = [12.96, 24.75, 32.75, 46.45, 65.56]\n",
        "\n",
        "x = np.arange(5)\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(x, p1, label='#workers=1')\n",
        "plt.plot(x, p2, label='#workers=2')\n",
        "plt.plot(x, p3, label='#workers=3')\n",
        "plt.plot(x, p4, label='#workers=4')\n",
        "\n",
        "plt.xticks(x, xticks)\n",
        "plt.xlabel('#videos')\n",
        "plt.ylabel('Inference Time (mins)')\n",
        "plt.legend(loc='best')\n",
        "\n",
        "# plt.show()\n",
        "\n",
        "plt.savefig('infer.pdf')\n",
        "plt.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
