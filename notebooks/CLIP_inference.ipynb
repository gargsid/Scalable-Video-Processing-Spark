{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoIcZoXgf_8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bff34c6e-0ceb-4624-df0b-457698f7a806"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.1)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-7y_bcq_m\n",
            "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-7y_bcq_m\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (4.64.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (0.13.1+cu113)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->clip==1.0) (4.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (2.23.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (7.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (3.0.4)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369409 sha256=d051709c82cff0c671d9918263dc30752cb56888a0390445483e91959dd794c8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5uako29o/wheels/fd/b9/c3/5b4470e35ed76e174bff77c92f91da82098d5e35fd5bc8cdac\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import clip\n",
        "import numpy as np\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "XrYb6TRjgVju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oW2sk5ZTge5A",
        "outputId": "ea19eb83-e071-4742-b954-f2263d12537e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up class feature vectors"
      ],
      "metadata": {
        "id": "iF_JPetqy1rQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classes_templates = {}\n",
        "classes_templates['cifar10_classes'] = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "classes_templates['cifar10_templates'] = [\n",
        "    'a photo of a {}.',\n",
        "    'a blurry photo of a {}.',\n",
        "    'a black and white photo of a {}.',\n",
        "    'a low contrast photo of a {}.',\n",
        "    'a high contrast photo of a {}.',\n",
        "    'a bad photo of a {}.',\n",
        "    'a good photo of a {}.',\n",
        "    'a photo of a small {}.',\n",
        "    'a photo of a big {}.',\n",
        "    'a photo of the {}.',\n",
        "    'a blurry photo of the {}.',\n",
        "    'a black and white photo of the {}.',\n",
        "    'a low contrast photo of the {}.',\n",
        "    'a high contrast photo of the {}.',\n",
        "    'a bad photo of the {}.',\n",
        "    'a good photo of the {}.',\n",
        "    'a photo of the small {}.',\n",
        "    'a photo of the big {}.',\n",
        "]\n",
        "classes_templates['kinetics_templates'] = [\n",
        "    'a photo of {}.',\n",
        "    'a photo of a person {}.',\n",
        "    'a photo of a person using {}.',\n",
        "    'a photo of a person doing {}.',\n",
        "    'a photo of a person during {}.',\n",
        "    'a photo of a person performing {}.',\n",
        "    'a photo of a person practicing {}.',\n",
        "    'a video of {}.',\n",
        "    'a video of a person {}.',\n",
        "    'a video of a person using {}.',\n",
        "    'a video of a person doing {}.',\n",
        "    'a video of a person during {}.',\n",
        "    'a video of a person performing {}.',\n",
        "    'a video of a person practicing {}.',\n",
        "    'a example of {}.',\n",
        "    'a example of a person {}.',\n",
        "    'a example of a person using {}.',\n",
        "    'a example of a person doing {}.',\n",
        "    'a example of a person during {}.',\n",
        "    'a example of a person performing {}.',\n",
        "    'a example of a person practicing {}.',\n",
        "    'a demonstration of {}.',\n",
        "    'a demonstration of a person {}.',\n",
        "    'a demonstration of a person using {}.',\n",
        "    'a demonstration of a person doing {}.',\n",
        "    'a demonstration of a person during {}.',\n",
        "    'a demonstration of a person performing {}.',\n",
        "    'a demonstration of a person practicing {}.',\n",
        "]\n",
        "classes_templates['kinetics_classes'] = ['air drumming', 'chasing', 'head stand', 'tackling', 'yoga']"
      ],
      "metadata": {
        "id": "0HGM_Cc3k_Fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_classes = []\n",
        "for dataset in ['cifar10', 'kinetics']:\n",
        "  for class_i in classes_templates[f'{dataset}_classes']:\n",
        "    for template_i in classes_templates[f'{dataset}_templates']:\n",
        "      all_classes.append(template_i.format(class_i))"
      ],
      "metadata": {
        "id": "iiTj4BaUnFCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_classes)  # num class vectors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMW5zdD-nNuB",
        "outputId": "ba894ebd-dc00-44a1-ae59-86635003d437"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "320"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuHeb9Vj0X9g",
        "outputId": "508657fb-25e0-44da-d48a-2988f214753a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 338M/338M [00:01<00:00, 263MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = clip.tokenize(all_classes).to(device)\n",
        "with torch.no_grad(): text_features = model.encode_text(text)"
      ],
      "metadata": {
        "id": "lYsFd9UZprn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_features.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYSNlnhSq8-s",
        "outputId": "b902c01b-f08b-46b3-d84a-ee5f031d6e3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([320, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load data and preprocess"
      ],
      "metadata": {
        "id": "GODmFboEzLi2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://images.pexels.com/photos/104827/cat-pet-animal-domestic-104827.jpeg > cat.png"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25Gd1xtMyeJ5",
        "outputId": "37439265-2ff0-46f8-fd8c-db98b78d55b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 1906k  100 1906k    0     0  13.6M      0 --:--:-- --:--:-- --:--:-- 13.6M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_PATH = 'cat.png'\n",
        "image = preprocess(Image.open(IMG_PATH)).unsqueeze(0).to(device)"
      ],
      "metadata": {
        "id": "T0KRsnjPgiOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZKX-quHwN4j",
        "outputId": "bc127cf4-67ad-4d17-944a-c6294ee516e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 3, 224, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 8 * 60  # frames in 8 min video @ 1 fps\n",
        "TOP_N = 5\n",
        "inp = torch.tile(image, (BATCH_SIZE, 1, 1, 1))"
      ],
      "metadata": {
        "id": "XPVooqV6wZej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zY7N5Pzwi1Q",
        "outputId": "d9222e72-b7ab-45c0-e80a-768f4ab9e4c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([480, 3, 224, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract features and calculate similarity"
      ],
      "metadata": {
        "id": "b7V_cXv-zQsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    logits_per_image, _ = model(inp, text)\n",
        "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "    top_n_class_ids = np.argsort(probs, axis=1)[:, -TOP_N:][:, ::-1]"
      ],
      "metadata": {
        "id": "tlo87L4PyEMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_n_class_ids  # top N (highest to lowest prob from left to right)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvbouqxsj6Kn",
        "outputId": "bf655090-2882-453a-c621-40696801978e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[54, 61, 63, 60, 70],\n",
              "       [54, 61, 63, 60, 70],\n",
              "       [54, 61, 63, 60, 70],\n",
              "       ...,\n",
              "       [54, 61, 63, 60, 70],\n",
              "       [54, 61, 63, 60, 70],\n",
              "       [54, 61, 63, 60, 70]])"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[all_classes[i] for i in top_n_class_ids[0]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dVnAHVQ_VIo",
        "outputId": "9f586634-686f-4afb-c2d0-73af70f733f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a photo of a cat.',\n",
              " 'a photo of a small cat.',\n",
              " 'a photo of the cat.',\n",
              " 'a good photo of a cat.',\n",
              " 'a photo of the small cat.']"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    }
  ]
}